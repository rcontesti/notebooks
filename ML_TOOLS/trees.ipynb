{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"Index\"></a>\n",
    "## Index \n",
    "[Back to Index](#Index)\n",
    "\n",
    "1[Gini](#Index)\n",
    "\n",
    "2.[Cart](#Cart)\n",
    "\n",
    "3.[Ensemble Methods (Random Forest)](#Ensemble) \n",
    "\n",
    "3.1  [Bagging](#Bagging)\n",
    "\n",
    "3.2 [Random Forest](#Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"Gini\"></a>\n",
    "# 1. Gini Impurity Index\n",
    "[Back to Index](#Index)\n",
    "\n",
    "Purity =0\n",
    "\n",
    "$$ G_i =1- \\sum^n_{k=1}p^2_{i,k}$$\n",
    "\n",
    "\n",
    "$$ p^2_{i,k} = \\frac{\\# \\ of\\ class\\ k\\ instances\\ in\\ node\\ i\\ of\\ the\\ tree}{\\# \\ instances\\ in\\ node\\ i\\ of\\ the\\ tree} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"Cart\"></a>\n",
    "# 2. Classification And Regression Tree (CART) algorithm\n",
    "[Back to Index](#Index)\n",
    "\n",
    "    1. Pick a feature k and threshold tk that minimize the cost function\n",
    "\n",
    "$$ J(k,t_k) = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right} $$\n",
    "\n",
    "\n",
    "    2. Repeat for each node recurseverly\n",
    "\n",
    "    3. Algorithm stops when it cannot reduce purity or reaches maximum death \n",
    "    \n",
    "    ( Hyperparameter). Other hyperparams that can be settled (Min leafs, min nodes, etc)\n",
    "    \n",
    "    (Excersice can also be done with regressions instead of classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=\"iris_tree.dot\",\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Convert to png the .dot plot generated\n",
    "!dot -Tpng iris_tree.dot -o iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![iris_tree](iris_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main issue with Decision Trees is that they are very sensitive to small variations in the training data. Imagine a TA-TE-TI grid with 3 different classes across one diagonal. You can go with two horizontal lines as separation or with 3 vertical lines as well both would work well.\n",
    "\n",
    "That is the reason why Random Forest is better pick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id=\"Ensemble\"></a>\n",
    "## 3. Ensemble Methods\n",
    "[Back to index](#Index)<a id=\" \"></a>\n",
    "\n",
    "**The idea is to average different trees (different data sets) (BAGGING) or algorithms (VOTING) through single vote election to clasify.** Even when each algo is weak, thate average performs really well.\n",
    "\n",
    "**Ensemble methods work best when the predictors are as independent from one another as possible**. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensembleâ€™s accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.888\n",
      "VotingClassifier 0.88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Run this code as many times as you want the accuracy of the Voting Classfifier will always be the best'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## VOTING METHODS\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "        estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "        voting='hard'\n",
    "    )\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)        \n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "\n",
    "\"Run this code as many times as you want the accuracy of the Voting Classfifier will always be the best\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Bagging\"></a>\n",
    "## 3.2  Bagging (Bootstrap Agregation)\n",
    "[Back to Index](#Index)\n",
    "\n",
    "Since by bootstrapping with replacement some data might not be sampled at all, we use that data, aka out of bag, to perfrom the evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89333333333333331"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(), n_estimators=500,\n",
    "        max_samples=100, bootstrap=True, n_jobs=-1\n",
    "    )\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "#Out of Bag Evaluation\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-fd500f4b2807>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-fd500f4b2807>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <a id=\" \"></a>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<a id=\" \"></a>\n",
    "## \n",
    "[](#)<a id=\" \"></a>\n",
    "\n",
    "<a id=\" \"></a>\n",
    "## \n",
    "[](#)<a id=\" \"></a>\n",
    "\n",
    "<a id=\" \"></a>\n",
    "## \n",
    "[](#)<a id=\" \"></a>\n",
    "\n",
    "<a id=\" \"></a>\n",
    "## \n",
    "[](#)<a id=\" \"></a>\n",
    "\n",
    "<a id=\" \"></a>\n",
    "## \n",
    "[](#)<a id=\" \"></a>\n",
    "\n",
    "<a id=\" \"></a>\n",
    "## \n",
    "[](#)<a id=\" \"></a>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
